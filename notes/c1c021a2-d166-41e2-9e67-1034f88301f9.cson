createdAt: "2020-04-28T19:05:10.937Z"
updatedAt: "2020-08-12T00:30:51.129Z"
type: "MARKDOWN_NOTE"
folder: "6827e53fd4576b1dc031"
title: "HarvardX: PH125.8 x"
tags: []
content: '''
  # HarvardX: PH125.8 x
  # Data Science: Machine Learning
  
  ## Introduction to Machine Learning: Section 1
  
  Notation
  - $X_1, .., X_p$ denote the features/predictors/covariates, $Y$ denotes the outcomes, and $Y$  denotes the predictions. $\\hat Y$ is the outcome, want to match the actual outcome.
  - Machine learning prediction tasks can be divided into categorical and continuous outcomes. We refer to these as classification and prediction, respectively.
  
  Example
  - $Y_i$ = an outcome for observation or index i
  - Use boldface for _$X_i$_ to distinguish the vector of predictors from the individual predictors (not bolded) $X_{i,1}, .., X_{i,784}$ (letters example)
  - When referring to an arbitrary set of features and outcomes, we drop the index i and use Y and bold X
  - Uppercase is used to refer to variables because we think of predictors as random variables
  - Lowercase is used to denote observed values, for example, _$X=x$_
  
  
  ## Basics of Evaluating Machine Learning Algorithms: Section 2
  
  Caret package, training and test sets, and overall accuracy
  - We know that we will not be able to predict y (sex) very accurately based on x (height) because male and female heights are not that different relative to within group variability, but can we do better than guessing?
    - The answer to this question, we need to quantify the definition of better.
    - Therefore, to mimic the ultimate evaluation process, we typically split the data into two and act as if we don't know the outcome for one of these two sets.
    - To mimic the ultimate evaluation process, we randomly split our data into two — a training set and a test set — and act as if we don’t know the outcome of the test set. 
    - We develop algorithms using only the training set; the test set is used only for evaluation.
  - The createDataPartition()  function from the caret package can be used to generate indexes for randomly splitting data.
  - The simplest evaluation metric for categorical outcomes is overall accuracy: the proportion of cases that were correctly predicted in the test set.
  - Trained a model to predict gender based off height using a training and test set!
  - 
  
  Basics of Evaluating Machine Learning Algorithms
  - Generally speaking, overall accuracy can be a deceptive measure
  - Construct a confusion matrix which tabulates each combination of prediction and actual value, can do in R table. Have a higher accuracy for males rather than females, more males in dataset than females, would show higher overall accuracy
    - So when computing overall accuracy, the high percentage of mistakes made for females is outweighed by the gains in correct calls for men.
    - This can actually be a big problem in machine learning.
    - If your training data is biased in some way, you are likely to develop an algorithm that are biased as well.
  - A general improvement to using overall accuracy is to study sensitivity and specificity separately. Sensitivity, also known as the true positive rate or recall, is the proportion of actual positive outcomes correctly identified as such. Specificity, also known as the true negative rate, is the proportion of actual negative outcomes that are correctly identified as such.
  - A confusion matrix tabulates each combination of prediction and actual value. You can create a confusion matrix in R using the table() function or the confusionMatrix() function from the caret package. 
  
  - Sensitivity = True positive / (true positive + false negative) OR the first column of the confusion matrix that are called positives. Also known as the True positive rate or Recall
  - Specificity = True negatives / (true negatives + false positives) OR the proportion of negatives, the second column f the confusion matrix, that are called negatives. This is also called the true negative rate. 
    - Another way to calculate specificity by: True positives/(true positives + false positives) OR the proportion of outcomes called positives, in the first row of our confusion matrix that are actually positives. AKA precision and AKA the positive predictive value.
  - Note that unlike the true positive rate and the true negative rate, precision depends on the prevalence, since higher prevalence implies you can get higher precision, even when guessing. 
  - Confusion matrix from R
  
  ```
  Confusion Matrix and Statistics
  
            Reference
  Prediction Female Male
      Female     48   32
      Male       71  374
                                            
                 Accuracy : 0.8038          
                   95% CI : (0.7672, 0.8369)
      No Information Rate : 0.7733          
      P-Value [Acc > NIR] : 0.0513022       
                                            
                    Kappa : 0.3671          
                                            
   Mcnemar's Test P-Value : 0.0001809       
                                            
              Sensitivity : 0.40336         
              Specificity : 0.92118         
           Pos Pred Value : 0.60000         
           Neg Pred Value : 0.84045         
               Prevalence : 0.22667         
           Detection Rate : 0.09143         
     Detection Prevalence : 0.15238         
        Balanced Accuracy : 0.66227         
                                            
         'Positive' Class : Female
  ```
         
  
  Balanced Accuracy and F1 score
  - One matric that is preferred over overall accuracy is the average of specificity and sensitivity, referred to as the balanced accuracy
  - Because specificity and sensitivity are rates, it is more appropriate to compute the harmonic average of specificity and sensitivity like this: $F1 score = \\frac{1}{\\frac{1}{2}(\\frac{1}{recall} + \\frac{1}{precision})}$
  - In fact, the F1-score, a widely used one-number summary, is the harmonic average of precision and recall. 
  
  
  Prevalence matters in practice
  - A machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. For example, if you develop an algorithm for disease diagnosis with very high sensitivity, but the prevalence of the disease is pretty low, then the precision of your algorithm is probably very low based on Bayes' theorem.
  
  
  ROC and precision-recall curves
  - ROC = receiver operating characteristic
    - Plots sensitivity versus 1 - specificity or the false positive rate
    - Weakness: they neither of the measures plotted depend on prevalence, in cases where prevalence matters, we may instead make a precision recall plot. In height and sex example, precision of just guessing is not high (25%), while using the height cutoff precision is a bit higher
  
  
  
  ## Section 2: Machine Learning Basics
  
  ### Conditional Probabilities 
  - Conditional probabilities for each class k: $pk(x) = Pr(Y=k|X=x), for k = 1,...,K$
  - Predictors are bolded, $p(x)$ = conditional probabilities as functions
  - In machine learning, this is referred to as Bayes' Rule. This is a theoretical rule because in practice we don't know $p(x)$. Having a good estimate of the $p(x)$ will suffice for us to build optimal prediction models, since we can control the balance between specificity and sensitivity however we wish. In fact, estimating these conditional probabilities can be thought of as the main challenge of machine learning. 
  
  ### Conditional expectations and loss function
  - Due to the connection between conditional probabilities and conditional expectations $pk(x) = Pr(Y=k|X=x), for k = 1,...,K$, we often only use the expectation to denote both the conditional probability and conditional expectation
  - For continuous outcomes, we define a loss function to evaluate the mode. The most commonly used one is MSE (mean squared error). The reason why we care about the conditional expectation in machine learning is that the expected vale minimizes the mean squared error $\\hat Y = E(Y|X=x)$ minimizes $E{(\\hat Y - Y)^2 | X=x}$. Due to this property, a succinct description of the main task of machine learning is that we use data to estimate for any set of features. The main way in which competing machine learning algorithms differ is in their approach to estimating this expectation.
  
  Comprehension check part 1
  
  What is the probability that a test is positive?
  $P(test+) = P(test+|disease) P(disease) + P(test+|healthy) P(healthy)$$
  
  What is the probability that an individual has the disease if the test is negative?
  $P(test-|disease) = P(test-|disease) * P(disease)/p(test-) = \\frac{P(teat-|disease) P(disease)}{p(test-|disease)P(disease)+P(test-|healthy)P(healthy)}$
  
  What is the probability that you have the disease if the test is positive?
  $𝑃(disease|test+)=𝑃(test+|disease)× \\frac{𝑃(disease)}{𝑃(test+)} = \\frac{𝑃(test+|disease)𝑃(disease)}{𝑃(test+|disease)𝑃(disease)+𝑃(test+|healthy)𝑃(healthy)}$
  
  Compare the prevalence of disease in people who test positive to the overall prevalence of disease.
  
  If a patient's test is positive, how much does that increase their risk of having the disease?
  First calculate the probability of having the disease given a positive test, then divide by the probability of having the disease.
  $\\frac{𝑃(test+|disease)𝑃(disease)}{𝑃(test+|disease)𝑃(disease)+𝑃(test+|healthy)𝑃(healthy)} = \\frac{0.147}{0.02}$
  
  Comprehension check part 2
   - all coded in R, code is on github
  
  ## Section 3: Linear Regression for Prediction, Smoothing, and working with matrices
  
  ### Linear Regression for Prediction
  - Linear regression can be considered a machine learning algorithm. Although it can be too rigid to be useful, it works rather well for some challenges. It also serves as a baseline approach: if you can’t beat it with a more complex approach, you probably want to stick to linear regression. 
  
  ### Predict function (in R)
  - Takes a fitted object from functions such as m or glm and a dataframe with the new predictors for which you want to precit ans returns a prediction. Same as using the regression formula. 
  
  Comprehenision check: linear regression
  - code on github
  
  ### Regression for a Categorical Outcome
  
  ### Logistic regression
  - Logistic regression is an extension of linear regression that assures that the estimate of conditional probability  𝑃𝑟(𝑌=1|𝑋=𝑥)  is between 0 and 1. This approach makes use of the logistic transformation: 
  $g(p)= log \\frac{p}{1-p}$
  - With logistic regression, we model the conditional probability directly with:
  $𝑔{𝑃𝑟(𝑌=1|𝑋=𝑥)}=𝛽0+𝛽1𝑥 $
  - Note that with this model, we can no longer use least squares. Instead we compute the maximum likelihood estimate (MLE). 
  - In R, we can fit the logistic regression model with the function glm() (generalized linear models). If we want to compute the conditional probabilities, we want type="response" since the default is to return the logistic transformed values.
  
  ### Case study: 2 or 7
  - In this case study we apply logistic regression to classify whether a digit is two or seven. We are interested in estimating a conditional probability that depends on two variables:
  - $g { p(x1, x2} = g{Pr(Y = 1|X_1 = x_1, X_2 = x_2)} = |beta_0+ \\beta_1 x_1+\\beta_2 x_2$
  - Through this case, we know that logistic regression forces our estimates to be a plane and our boundary to be a line. This implies that a logistic regression approach has no chance of capturing the non-linear nature of the true  𝑝(𝑥1,𝑥2) . Therefore, we need other more flexible methods that permit other shapes.
  
  Comprehension check: Logistic regression 
  - code on github
  
'''
linesHighlighted: []
isStarred: false
isTrashed: false
